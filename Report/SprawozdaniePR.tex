\documentclass[12pt,a4paper]{article}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{pslatex} %z tym czcionka wygląda ładniej

\usepackage{xcolor}
\definecolor{CodeListingColor}{rgb}{0.95,0.95,0.95}
\usepackage{minted}

\usepackage{xpatch}
\xpretocmd{\inputminted}{\par\vspace{-1em}}{}{}
\xapptocmd{\inputminted}{\par\vspace{-1em}}{}{}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{amsfonts}
\usepackage{csvsimple}
%
\usepackage{adjustbox}
\usepackage{enumitem}

\setlength\parindent{0pt} %żeby wcięć przed akapitem nie było
\usepackage{parskip} %no need to use \\ to add empty line between paragraphs any more

%\author{
%  Ewa Fengler 132219
%  \and
%  Dariusz Grynia 132235
%  \and
%  gr. I1, wt. godz. 15.10, tyg. parzyste
%}
\date{}
\title{Przetwarzanie równoległe \\ \Large Projekt 2 CUDA}

\usepackage[a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm, headsep=1.2cm]{geometry}
\usepackage[figurename=Rys.]{caption}
\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{float}
%\usepackage{etoolbox}
%\makeatletter
%\patchcmd{\Ginclude@eps}{"#1"}{#1}{}{}
%\makeatother

%\linespread{1.1}

\begin{document}
\maketitle
\thispagestyle{empty}

\vspace{-2cm}
\section{Wstęp}

\subsection{Autorzy}
Ewa Fengler 132219\\
Dariusz Grynia 132235\\
grupa I1,\\
wtorki godz. 15.10,\\
tygodnie parzyste\\

\subsection{Adres kontaktowy}
dariusz.grynia@student.put.poznan.pl

\subsection{Temat zadania}
Ukrycie kosztów transferu danych w czasie obliczeń.

Porównanie wersji kodu:
\begin{itemize}
\item [3.] grid wieloblokowy, obliczenia przy wykorzystaniu pamięci współdzielonej bloku wątków
\item [5.] grid wieloblokowy, obliczenia przy wykorzystaniu pamięci współdzielonej bloku wątków, zrównoleglenie obliczeń i transferu danych między pamięciami: operacyjną procesora, a globalną karty
\end{itemize}

\subsection{Opis wykorzystanej karty graficznej}

TODO

\section{Analiza z przygotowania eksperymentu}

\subsection{Mnożenie macierzy z wykorzystaniem karty graficznej}

Mnożenie macierzy jest procesem kosztownym obliczeniowo i przez to czasochłonnym. Jednym z rozwiązań mających na celu skrócenie czasu przetwarzania jest podział pracy oraz zrównoleglenie obliczeń. W przypadku obliczeń z wykorzystaniem karty graficznej, w przeciwieństwie do obliczeń na procesorze wielordzeniowym ogólnego przeznaczenia, efektywne przetwarzanie wymaga dostosowania algorytmu mnożenia tak, aby wykorzystywał bardzo dużą liczbę wątków.

Możenie macierzy polega obliczaniu jednej komórki macierzy wynikowej przez jeden wątek. Na pojedynczym multiprocesorze jednocześnie przetwarzane są wątki jednej wiązki. Wykonują one zawsze w danym momencie tę samą instrukcję, lecz na innych danych. Nie zawsze wątki są gotowe do obliczeń, np. w trakcie oczekiwania na dane. Wtedy sprzętowy moduł szeregujący wątki przełącza kontekst i następuje przetwarzanie gotowych wątków innej wiązki (z tego samego lub innego bloku wątków).

W trakcie całego procesu mnożenia macierzy, wykorzystywanych jest $n^2$ wątków (n -- jeden wymiar macierzy kwadratowej). Wątki są pogrupowane w bloki, te natomiast składają się na strukturę zwaną gridem. Taka organizacja umożliwia z jednej strony efektywne szeregowanie obliczeń wykonywanych na karcie graficznej, z drugiej strony pozwala programiście kontrolować na rzecz jakich danych wątki wykonują instrukcje, poprzez wykorzystanie identyfikatorów bloków oraz wątków wewnątrz bloku np. do indeksowania tablic.

\subsection{Dostęp do pamięci}

Prędkość przetwarzania na procesorze ogólnego przeznaczenia w dużym stopniu zależała od efektywności dostępu do danych. Dostęp do pamięci operacyjnej cechował się stosunkowo dużym opóźnieniem, dlatego duże znaczenie miało efektywne wykorzystanie pamięci podręcznej.
W przypadku karty graficznej, dane mogą być przechowywane w stosunkowo powolnej pamięci globalnej. Opóźnienia są w tym przypadku bardzo znaczące i wynoszą 200 cykli procesora. W celu zwiększenia efektywności przetwarzania, należy wykorzystać odpowiednio dużą liczbę wątków, tak aby zawsze jakaś wiązka była gotowa do obliczeń, podczas gdy inne czekają na dane. Niestety z powodu ograniczeń na maksymalną liczbę wątków na multiprocesor, nadal nie jest możliwe zapewnienie ciągłości obliczeń.

W realizowanym temacie zostało wykorzystane inne podejście -- wykorzystanie pamięci współdzielonej, która jest znacznie szybsza od pamięci globalnej. Czas dostępu do danych znajdujących się w pamięci współdzielonej jest w przybliżeniu 100 razy krótszy niż w przypadku pamięci globalnej (pod warunkiem, że nie ma konfliktu dostępu do tych samych banków pamięci współdzielonej). Do danych znajdujących się w pamięci współdzielonej mają dostęp wszystkie wątki w ramach bloku. Zanim jednak będą mogły z nich korzystać, konieczne jest skopiowanie odpowiednich danych z pamięci globalnej do pamięci współdzielonej. W celu zwiększenia efektywności, dostępy do pamięci globalnej mogą być łączone w transakcje. Jednak aby było to możliwe, konieczne jest spełnienie następującego warunku: wątki w ramach pół-warpu muszą jednocześnie odwoływać się do sąsiednich adresów pamięci.


\subsection{Kod}

% set options once for all listings
\setminted{
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	tabsize=2,
	bgcolor=CodeListingColor,
	%fontsize=\footnotesize,
	breaklines,
	linenos %Enables line numbers
}

\begin{listing}[H]
\inputminted{cuda}{listings/kernel.cu}
\caption{Kod kernela, obliczenia przy wykorzystaniu pamięci współdzielonej bloku wątków}
\label{lst:kernel}
\end{listing}

\newpage
Kod źródłowy przedstawiony na listingu \ref{lst:kernel} to funkcja -- kernel, uruchamiany na karcie graficznej. Początkowe linie (4 - 13) służą wyznaczeniu przechowywanych w rejestrach wartości, wykorzystywanych dalej do indeksowania oraz sterowania pętlą. Następnie ma miejsce deklaracja tymczasowej zmiennej akumulującej obliczane iloczyny odpowiednich elementów macierzy. Pętla obejmujące linie 16 - 32 służy iteracji po kolejnych blokach macierzy A i B. Jeden blok wątków oblicza jeden blok macierzy wynikowej, jednak potrzebuje kolejno wszystkich bloków macierzy źródłowych, w celu wyznaczenia pełnego wyniki (analogicznie do zewnętrznych pętli metody 6-pętlowej dla CPU).
Linie 20-23 służą deklaracji oraz pobraniu danych z pamięci globalnej do pamięci współdzielonej. Wątki w ramach połowy warpu odwołują się do sąsiednich komórek macierzy (wartości tx są kolejnymi liczbami), co pozwala na efektywny, łączony dostęp do pamięci globalnej. W linii 24 ma miejsce synchronizacja wątków całego bloku. Jeśli w pamięci współdzielonej znajdowałby się tylko blok macierzy A, natomiast dane z macierzy B wątki odczytywałyby z pamięci synchronizacja byłaby zbędna, ponieważ każda wiązka korzystałaby tylko z danych, które sama wcześniej pobrała, a wszystkie wątki wiązki wykonują w danym czasie tą samą instrukcję (kod nie zawiera żadnych rozgałęzień a więc nie ma tutaj rozbieżności wątków). W przypadku macierzy B, wątki danej wiązki odczytują jednak również dane, które są wczytywane przez pozostałe wiązki (odczyt po kolumnie), zatem synchronizacja jest konieczna, ponieważ wszystkie wiązki muszą pobrać dane, zanim którakolwiek będzie mogła rozpocząć obliczenia. Synchronizacja jest konieczna, jednak do pewnego stopnia ogranicza wydajność, ponieważ nie mam możliwości jednoczesnego pobierania danych i wykonywania obliczeń przez różne wiązki bloku. Możliwa jest natomiast realizacja przetwarzania przez wątki z innych bloków, przydzielonych na dany multiprocesor, jednak ich liczba jest ograniczona, przez co karta graficzna może nie wykonywać obliczeń przez $100\%$ czasu.
Następnie w linii 29 ma miejsce faktyczne mnożenie macierzy. Jeden wątek oblicza jeden wynik, wykorzystując wiersz bloku macierzy A oraz kolumnę bloku macierzy B. Dyrektywa \verb|#pragma unroll| powoduje rozwinięcie pętli, co pozwala wyeliminować narzut wydajnościowy, który wynikałby ze sprawdzania warunku oraz inkrementacji zmiennej sterującej.
Po wykonaniu obliczeń, konieczna jest również synchronizacja, aby wątki, należące do wiązek, które skończyły już obliczenia, nie mogły pobierać nowych danych do pamięci współdzielonej, w czasie kiedy inne wiązki jeszcze wykorzystują zapisane wcześniej w tym miejscu dane do obliczeń. Ostatecznie w linii 34 wynik przechowywany w zmiennej tymczasowej jest zapisywany macierzy C w pamięci globalnej pod odpowiednim indeksem. Tutaj również wątki odwołują się do kolejnych adresów, zatem możliwe jest połączenie danych zapisywanych przez wątki z połowy warpu w jedną transakcję.\\

\textit{TODO to powyżej jakoś sensownie podzielić na akapity}\\
\textbf{TODO opis obu wywołań}\\
\textbf{TODO rysunki: przesyłanie sync/async}

\textbf{rysunki z opisem określające:}
\begin{itemize}
\item miejsce dostępu i kolejność dostępu do danych realizowane przez poszczególne wątki, bloki
\item wyznaczane przez wątki i bloki wartości wyników
\end{itemize}

\begin{listing}[H]
\inputminted{cuda}{listings/invocation_sync.cu}
\caption{Wywołanie kernela, wersja 3}
\label{lst:sync}
\end{listing}

Listing 2 przedstawia wywołanie kernela dla wersji trzeciej kodu. Zmienna \verb|nStreams| określa w tym przypadku ile macierzy będzie obliczanych, jednak przesyłanie macierzy wejściowych do pamięci karty graficznej, obliczenia oraz kopiowanie wyników do pamięci operacyjnej następują sekwencyjnie, po zakończeniu poprzedniej operacji. Do przechowywania wskaźników do segmentów pamięci zawierających dane wszystkich przetwarzanych macierzy wykorzystano wektory \verb|h_A, h_B, h_C|, zawierające wskaźniki do pamięci operacyjnej oraz \verb|d_A, d_B, d_C|, zawierające wskaźniki do danych w pamięci karty graficznej. Pamięć na karcie graficznej została wcześniej zaalokowana z wykorzystaniem funkcji \verb|cudaMalloc|. Poprawność wywołania wszystkich funkcji była sprawdzana, jednak, w celu zwiększenia czytelności, listingi 2 i 3 zawierają jedynie niezbędny do opisu koncepcji kod. Pełna wersja kodu, dołączona do sprawozdania, zawiera wszystkie szczegóły.

Na listingu 3 przedstawiono wywołanie kernela dla piątej wersji kodu, w której obliczenia na karcie graficznej zrównoleglono z przesyłaniem danych pomiędzy pamięcią karty a pamięcią operacyjną. W tym celu w liniach 1-4 zadeklarowano oraz zainicjowano strumienie, w liczbie równej liczbie obliczanych macierzy. Pamięć na karcie graficznej tutaj również zaalokowano wcześniej za pomocą funkcji \verb|cudaMalloc|, jednak w odróżnieniu od poprzedniej wersji, macierze znajdujące się w pamięci operacyjnej zaalokowano z wykorzystaniem funkcji \verb|cudaMallocHost|. Funkcja ta pozwala na alokację pamięci z wyłączonym stronicowaniem, co jest wymagane w przypadku asynchronicznych transferów wykonywanych przez funkcję \verb|cudaMemcpyAsync|. Kolejne pętle służą zakolejkowaniu operacji kopiowania danych do pamięci karty, wywołań kernela oraz kopiowania wyników do pamięci operacyjnej. Warto wspomnieć o każdorazowym przypisaniu danej operacji do strumienia, oraz o tym, że funkcje te są nieblokujące, oznaczają tylko zlecenie pewnej operacji, która jest wykonywana w tle. W ramach jednego strumienia operacje są oczywiście wykonywane sekwencyjnie, co gwarantuje, że dane wykorzystywane przez dany kernel zostaną przesłane przed jego uruchomieniem, natomiast wyniki zostaną przesłane do pamięci operacyjnej dopiero gdy przetwarzanie kernala się zakończy. Zysk wynikający ze zwrónoleglenia transferu danych będzie zatem widoczny w przypadku, gdy wykorzystany zostanie więcej niż jeden strumień.

W odróżnieniu od poprzedniej wersji, wykorzystano tutaj oddzielne pętle, służące do kolejkowania transferu danych oraz wywołań kernela. Ma to na celu zakolejkowanie najpierw wszystkich operacji przesyłania danych do karty, następnie wszystkich wywołań kerneli i na końcu wszystkich operacji kopiowania wyników obliczeń. W przypadku wykorzystania jednej pętli, wymienione operacje byłyby kolejkowane na przemian. Kolejność kolejkowania ma ogromne znaczenie w przypadku pracy z kartami graficznymi ze starszych generacji. W przypadku wykorzystywanej karty graficznej, GTX 260, compute capability 1.3, jedna jednostka kopiująca dane, operacje powinny być kolejkowane tak jak przedstawiono na listingu 3. W przypadku wykorzystania jednej pętli, mechanizm kolejkujący karty graficznej nie byłby w stanie zrównoleglić operacji. Zupełnie inaczej sytuacja wygląda w przypadku kart graficznych z compute capability 2.0, kiedy dostępne są dwie jednostki kopiujące dane (jedna do pamięci karty graficznej, druga z pamięci karty graficznej do pamięci operacyjnej). W takim przypadku, oddzielne zakolejkowanie operacji w trzech pętlach umożliwiłoby jednoczesne kopiowanie danych do pamięci karty oraz wykonywanie obliczeń, jednak kopiowanie wyników następowałoby dopiero po skończeniu przetwarzania kernela ostatniego strumienia, co nie byłoby optymalne, ponieważ druga jednostka kopiująca, mogłaby kopiować dane strumienia który zakonćzył już obliczenia w trakcie gdy trwa przetwarzanie następnego. Aby efektywnie wykorzystać możliwości takiej karty graficznej konieczne byłoby zastosowanie naprzemiennego kolejkowania operacji (jedna pętla).

Powyższe uwagi nie dotyczą nowszych kart graficznych, obsługujących co najmniej compute capability 3.5, w przypadku których kolejność zlecania operacji nie ma znaczenia, ponieważ system zarządzający potrafi uszeregować te operacje w optymalny sposób. Nowsze karty graficzne oferują także wiele innych usprawnień, m.in. zaawansowany mechanizm dostępu do pamięci globalnej, zmniejszający straty wydajności, w przypadkach gdzie dostęp do pamięci globalnej na kartach z compute capability nie mógł być łączony (z powodu nieodpowiedniego adresowania).


\begin{listing}[H]
\inputminted{cuda}{listings/invocation_async.cu}
\caption{Wywołanie kernela, wersja 5, zrównoleglenie obliczeń i transferu danych}
\label{lst:async}
\end{listing}


%\section{Eksperyment pomiarowy}
%TODO
%
%\subsection{Instancje}
%
%TODO
%
%\subsection{Mierzone parametry}
%
%TODO
%
%\section{Wnioski}
%
%TODO

\end{document}