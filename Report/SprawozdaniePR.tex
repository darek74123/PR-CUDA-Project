\documentclass[12pt,a4paper]{article}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{pslatex} %z tym czcionka wygląda ładniej

\usepackage{xcolor}
\definecolor{CodeListingColor}{rgb}{0.95,0.95,0.95}
\usepackage{minted}

\usepackage{xpatch}
\xpretocmd{\inputminted}{\par\vspace{-1em}}{}{}
\xapptocmd{\inputminted}{\par\vspace{-1em}}{}{}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{amsfonts}
\usepackage{csvsimple}
%
\usepackage{adjustbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumitem}
\makeatletter
\newcommand{\mtmathitem}{%
\xpatchcmd{\item}{\@inmatherr\item}{\relax\ifmmode$\fi}{}{\errmessage{Patching of \noexpand\item failed}}
\xapptocmd{\@item}{$}{}{\errmessage{appending to \noexpand\@item failed}}}
\makeatother

\newenvironment{mathitem}[1][]{%
\itemize[#1]\mtmathitem}{$\endlist}                    %$

\newenvironment{mathenum}[1][]{%
\enumerate[#1]\mtmathitem}{$\endlist}                  %$

\newenvironment{mathdesc}[1][]{%
\description[#1]\mtmathitem}{$\endlist}                %$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[compact]{titlesec}
\titlespacing{\subsubsection}{0pt}{1em}{0em}
\setlength\parindent{0pt} %żeby wcięć przed akapitem nie było
\usepackage{parskip} %no need to use \\ to add empty line between paragraphs any more

%\author{
%  Ewa Fengler 132219
%  \and
%  Dariusz Grynia 132235
%  \and
%  gr. I1, wt. godz. 15.10, tyg. parzyste
%}
\date{}
\title{Przetwarzanie równoległe \\ \Large Projekt 2 CUDA}

\usepackage[a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm, headsep=1.2cm]{geometry}
\usepackage[figurename=Rys.]{caption}
\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{float}
%\usepackage{etoolbox}
%\makeatletter
%\patchcmd{\Ginclude@eps}{"#1"}{#1}{}{}
%\makeatother

%\linespread{1.1}

\begin{document}
\maketitle
\thispagestyle{empty}

\vspace{-2cm} %TODO ustawić na ile będzie trzeba, kiedy już sprawko będzie miało całą zawartość
\section{Wstęp}

\subsection{Autorzy}
Ewa Fengler 132219\\
Dariusz Grynia 132235\\
grupa I1,\\
wtorki godz. 15.10,\\
tygodnie parzyste\\

\subsection{Adres kontaktowy}
dariusz.grynia@student.put.poznan.pl

\subsection{Temat zadania}
Ukrycie kosztów transferu danych w czasie obliczeń.

Porównanie wersji kodu:
\begin{itemize}
\item [3.] grid wieloblokowy, obliczenia przy wykorzystaniu pamięci współdzielonej bloku wątków
\item [5.] grid wieloblokowy, obliczenia przy wykorzystaniu pamięci współdzielonej bloku wątków, zrównoleglenie obliczeń i transferu danych między pamięciami: operacyjną procesora a globalną karty
\end{itemize}

\subsection{Opis wykorzystanej karty graficznej}

TODO

\textit{opis wykorzystywanej karty graficznej (typ, CC – możliwości obliczeniowe, liczba SM i rdzeni, innych
jednostek obliczeniowych, ograniczenia dla tego CC)}

\section{Analiza z przygotowania eksperymentu}

\subsection{Mnożenie macierzy z wykorzystaniem karty graficznej}

Mnożenie macierzy jest procesem kosztownym obliczeniowo i przez to czasochłonnym. Jednym z rozwiązań mających na celu skrócenie czasu przetwarzania jest podział pracy oraz zrównoleglenie obliczeń. W przypadku obliczeń z wykorzystaniem karty graficznej, w przeciwieństwie do obliczeń na procesorze wielordzeniowym ogólnego przeznaczenia, efektywne przetwarzanie wymaga dostosowania algorytmu mnożenia tak, aby wykorzystywał bardzo dużą liczbę wątków.

Mnożenie macierzy polega na obliczaniu jednej komórki macierzy wynikowej przez jeden wątek. Na pojedynczym multiprocesorze jednocześnie przetwarzane są wątki jednej wiązki. Wykonują one zawsze w danym momencie tę samą instrukcję, lecz na innych danych. Nie zawsze wątki są gotowe do obliczeń, np. w trakcie oczekiwania na dane. Wtedy sprzętowy moduł szeregujący wątki przełącza kontekst i następuje przetwarzanie gotowych wątków innej wiązki (z tego samego lub innego bloku wątków).

W trakcie całego procesu mnożenia macierzy, wykorzystywanych jest $n^2$ wątków (n -- jeden wymiar macierzy kwadratowej). Wątki są pogrupowane w bloki, te natomiast składają się na strukturę zwaną gridem. Taka organizacja umożliwia z jednej strony efektywne szeregowanie obliczeń wykonywanych na karcie graficznej, z drugiej strony pozwala programiście kontrolować na rzecz jakich danych wątki wykonują instrukcje, poprzez wykorzystanie identyfikatorów bloków oraz wątków wewnątrz bloku, np. do indeksowania tablic.

\subsection{Dostęp do pamięci}

Prędkość przetwarzania na procesorze ogólnego przeznaczenia w dużym stopniu zależała od efektywności dostępu do danych. Dostęp do pamięci operacyjnej cechował się stosunkowo dużym opóźnieniem, dlatego duże znaczenie miało efektywne wykorzystanie pamięci podręcznej.
W przypadku karty graficznej, dane mogą być przechowywane w stosunkowo powolnej pamięci globalnej. Opóźnienia są w tym przypadku bardzo znaczące i wynoszą 200 cykli procesora. W celu zwiększenia efektywności przetwarzania, należy wykorzystać odpowiednio dużą liczbę wątków, tak aby zawsze jakaś wiązka była gotowa do obliczeń, podczas gdy inne czekają na dane. Niestety z powodu ograniczeń na maksymalną liczbę wątków na multiprocesor, nadal nie jest możliwe zapewnienie ciągłości obliczeń.

W realizowanym temacie zostało wykorzystane inne podejście -- wykorzystanie pamięci współdzielonej, która jest znacznie szybsza od pamięci globalnej. Czas dostępu do danych znajdujących się w pamięci współdzielonej jest w przybliżeniu 100 razy krótszy niż w przypadku pamięci globalnej (pod warunkiem, że nie ma konfliktu dostępu do tych samych banków pamięci współdzielonej). Do danych znajdujących się w pamięci współdzielonej mają dostęp wszystkie wątki w ramach bloku. Zanim jednak będą mogły z nich korzystać, konieczne jest skopiowanie odpowiednich danych z pamięci globalnej do pamięci współdzielonej. W celu zwiększenia efektywności, dostępy do pamięci globalnej mogą być łączone w transakcje. Jednak aby było to możliwe, konieczne jest spełnienie następującego warunku: wątki w ramach pół-warpu muszą jednocześnie odwoływać się do sąsiednich adresów pamięci.


\subsection{Kod}

% set options once for all listings
\setminted{
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	tabsize=2,
	bgcolor=CodeListingColor,
	%fontsize=\footnotesize,
	breaklines,
	linenos %Enables line numbers
}

\begin{listing}[H]
\inputminted{cuda}{listings/kernel.cu}
\caption{Kod kernela, obliczenia przy wykorzystaniu pamięci współdzielonej bloku wątków}
\label{lst:kernel}
\end{listing}

\newpage
Kod źródłowy przedstawiony na listingu \ref{lst:kernel} to funkcja -- kernel, uruchamiany na karcie graficznej. Początkowe linie (4 - 13) służą wyznaczeniu przechowywanych w rejestrach wartości, wykorzystywanych dalej do indeksowania oraz sterowania pętlą. Następnie ma miejsce deklaracja tymczasowej zmiennej akumulującej obliczane iloczyny odpowiednich elementów macierzy. Pętla obejmujące linie 16 - 32 służy iteracji po kolejnych blokach macierzy A i B. Jeden blok wątków oblicza jeden blok macierzy wynikowej, jednak potrzebuje kolejno wszystkich bloków macierzy źródłowych, w celu wyznaczenia pełnego wyniku (analogicznie do zewnętrznych pętli metody 6-pętlowej dla CPU).

Linie 20-23 służą deklaracji oraz pobraniu danych z pamięci globalnej do pamięci współdzielonej. Wątki w ramach połowy warpu odwołują się do sąsiednich komórek macierzy (wartości \verb|tx| są kolejnymi liczbami), co pozwala na efektywny, łączony dostęp do pamięci globalnej.

W linii 24 ma miejsce synchronizacja wątków całego bloku. Gdyby w pamięci współdzielonej znajdował się tylko blok macierzy A, natomiast dane z macierzy B wątki odczytywałyby każdorazowo z pamięci globalnej, synchronizacja byłaby zbędna, ponieważ każda wiązka korzystałaby tylko z danych, które sama wcześniej pobrała. Wszystkie wątki wiązki wykonują w danym czasie tą samą instrukcję, kod nie zawiera żadnych rozgałęzień, a więc nie ma tutaj rozbieżności wątków. W przypadku omawianego kodu, w pamięci współdzielonej przechowywany jest także blok macierzy B.
Wątki danej wiązki odczytują kolejne wartości iterując po kolumnie macierzy B, a zatem korzystają również z danych, które są wczytywane przez pozostałe wiązki. Skutkiem tego synchronizacja jest konieczna, ponieważ wszystkie wiązki muszą uzupełnić blok macierzy B, zanim którakolwiek będzie mogła rozpocząć obliczenia. Wymagana synchronizacja do pewnego stopnia ogranicza wydajność, ponieważ nie ma możliwości jednoczesnego pobierania danych i wykonywania obliczeń przez różne wiązki bloku. Możliwa jest natomiast realizacja przetwarzania innego bloku wątków, przydzielonego na dany multiprocesor, w trakcie oczekiwania na dane przez tamten blok. Liczba bloków przypadających na multiprocesor jest jednak ograniczona, przez co karta graficzna może nie wykonywać obliczeń przez $100\%$ czasu.

Następnie w linii 29 ma miejsce faktyczne mnożenie macierzy. Jeden wątek oblicza jeden wynik, wykorzystując wiersz bloku macierzy A oraz kolumnę bloku macierzy B. Dyrektywa \verb|#pragma unroll| powoduje rozwinięcie pętli, co pozwala wyeliminować narzut wydajnościowy, który wynikałby ze sprawdzania warunku oraz inkrementacji zmiennej sterującej.

Po wykonaniu obliczeń, konieczna jest również synchronizacja, aby wątki należące do wiązek, które skończyły już obliczenia, nie mogły pobierać nowych danych do pamięci współdzielonej, w czasie kiedy inne wiązki jeszcze wykorzystują pobrany wcześniej fragment macierzy B do obliczeń.

Ostatecznie w linii 34 wynik przechowywany w zmiennej tymczasowej jest zapisywany macierzy C w pamięci globalnej pod odpowiednim indeksem. Tutaj również wątki odwołują się do kolejnych adresów, zatem możliwe jest połączenie danych zapisywanych przez wątki z połowy warpu w jedną transakcję.\\

TODO to powyżej jakoś sensownie podzielić na akapity\\
\textbf{TODO rysunki: przesyłanie sync/async}

\textit{\textbf{rysunki z opisem określające:}}
\textit{\begin{itemize}
\item miejsce dostępu i kolejność dostępu do danych realizowane przez poszczególne wątki, bloki
\item wyznaczane przez wątki i bloki wartości wyników
\end{itemize}}

\begin{listing}[H]
\inputminted{cuda}{listings/invocation_sync.cu}
\caption{Wywołanie kernela, wersja 3}
\label{lst:sync}
\end{listing}

Listing 2 przedstawia wywołanie kernela dla wersji trzeciej kodu. Zmienna \verb|nStreams| określa w tym przypadku ile macierzy będzie obliczanych, jednak przesyłanie macierzy wejściowych do pamięci karty graficznej, obliczenia oraz kopiowanie wyników do pamięci operacyjnej następują sekwencyjnie, po zakończeniu poprzedniej operacji. Do przechowywania wskaźników do segmentów pamięci zawierających dane wszystkich przetwarzanych macierzy wykorzystano wektory \verb|h_A, h_B, h_C|, zawierające wskaźniki do pamięci operacyjnej oraz \verb|d_A, d_B, d_C|, zawierające wskaźniki do danych w pamięci karty graficznej. Pamięć na karcie graficznej została wcześniej zaalokowana z wykorzystaniem funkcji \verb|cudaMalloc|. Poprawność wywołania wszystkich funkcji była sprawdzana, jednak, w celu zwiększenia czytelności, listingi 2 i 3 zawierają jedynie niezbędny do opisu koncepcji kod. Pełna wersja kodu, dołączona do sprawozdania, zawiera wszystkie szczegóły.

Na listingu 3 przedstawiono wywołanie kernela dla piątej wersji kodu, w której obliczenia na karcie graficznej zrównoleglono z przesyłaniem danych pomiędzy pamięcią karty a pamięcią operacyjną. W tym celu w liniach 1-4 zadeklarowano oraz zainicjowano strumienie, w liczbie równej liczbie obliczanych macierzy. Pamięć na karcie graficznej tutaj również zaalokowano wcześniej za pomocą funkcji \verb|cudaMalloc|, jednak w odróżnieniu od poprzedniej wersji, macierze znajdujące się w pamięci operacyjnej zaalokowano z wykorzystaniem funkcji \verb|cudaMallocHost|. Funkcja ta pozwala na alokację pamięci z wyłączonym stronicowaniem, co jest wymagane w przypadku asynchronicznych transferów wykonywanych przez funkcję \verb|cudaMemcpyAsync|. Kolejne pętle służą zakolejkowaniu operacji kopiowania danych do pamięci karty, wywołań kernela oraz kopiowania wyników do pamięci operacyjnej. Warto wspomnieć o każdorazowym przypisaniu danej operacji do strumienia, oraz o tym, że funkcje te są nieblokujące, oznaczają tylko zlecenie pewnej operacji, która jest wykonywana w tle. W ramach jednego strumienia operacje są oczywiście wykonywane sekwencyjnie, co gwarantuje, że dane wykorzystywane przez dany kernel zostaną przesłane przed jego uruchomieniem, natomiast wyniki zostaną przesłane do pamięci operacyjnej dopiero, gdy przetwarzanie kernela się zakończy. Zysk wynikający ze zrównoleglenia transferu danych będzie zatem widoczny w przypadku, gdy wykorzystany zostanie więcej niż jeden strumień.

W odróżnieniu od poprzedniej wersji, wykorzystano tutaj oddzielne pętle, służące do kolejkowania transferu danych oraz wywołań kernela. Ma to na celu zakolejkowanie najpierw wszystkich operacji przesyłania danych do karty, następnie wszystkich wywołań kerneli i na końcu wszystkich operacji kopiowania wyników obliczeń. W przypadku wykorzystania jednej pętli, wymienione operacje byłyby kolejkowane na przemian. Kolejność kolejkowania ma ogromne znaczenie w przypadku pracy z kartami graficznymi ze starszych generacji. W przypadku wykorzystywanej karty graficznej, GTX 260, compute capability 1.3, jedna jednostka kopiująca dane, operacje powinny być kolejkowane tak jak przedstawiono na listingu 3. W przypadku wykorzystania jednej pętli, mechanizm kolejkujący karty graficznej nie byłby w stanie zrównoleglić operacji.

Zupełnie inaczej sytuacja wygląda w przypadku kart graficznych z compute capability 2.0, kiedy dostępne są dwie jednostki kopiujące dane (jedna do pamięci globalnej karty, druga z pamięci karty graficznej do pamięci operacyjnej). W takim przypadku, oddzielne zakolejkowanie operacji w trzech pętlach umożliwiłoby jednoczesne kopiowanie danych do pamięci karty oraz wykonywanie obliczeń, jednak kopiowanie wyników następowałoby dopiero po skończeniu przetwarzania kernela ostatniego strumienia. Nie byłoby to optymalne, ponieważ druga jednostka kopiująca mogłaby kopiować dane strumienia, który zakończył już obliczenia w trakcie przetwarzania obliczeń następnego strumienia. Aby efektywnie wykorzystać możliwości takiej karty graficznej konieczne byłoby zastosowanie naprzemiennego kolejkowania operacji (jedna pętla).

Powyższe uwagi nie dotyczą nowszych kart graficznych, obsługujących co najmniej compute capability 3.5, w przypadku których kolejność zlecania operacji nie ma znaczenia, ponieważ system zarządzający potrafi uszeregować te operacje w optymalny sposób. Nowsze karty graficzne oferują także wiele innych usprawnień, m.in. zaawansowany mechanizm dostępu do pamięci globalnej, zmniejszający straty wydajności, w przypadkach gdzie dostęp do pamięci globalnej na kartach z compute capability nie mógł być łączony (z powodu nieodpowiedniego adresowania).


\begin{listing}[H]
\inputminted{cuda}{listings/invocation_async.cu}
\caption{Wywołanie kernela, wersja 5, zrównoleglenie obliczeń i transferu danych}
\label{lst:async}
\end{listing}


\section{Eksperyment pomiarowy}
TODO

\subsection{Instancje}

\subsubsection*{Rozmiar macierzy}
W zadaniu wykorzystano macierze kwadratowe o następujących rozmiarach (n - wielkość jednego wymiaru):
\begin{itemize}
\item n = 224
\item n = 1120
\item n = 864
\item n = 1728
\end{itemize}

\subsubsection*{Wielkość bloku wątków}

W celu zbadania wpływu rozmiaru bloku na czas obliczeń każda instancja została uruchomiona dla wielkości bloków:
\begin{itemize}
\item BS = 8 x 8
\item BS = 16 x 16
\end{itemize}
{\footnotesize Przyjęto oznaczenie BS, które będzie dalej wykorzystywane we wzorach i wnioskach.}

Wielkości instancji dobrano tak, aby zapewnić zrównoważone obciążenie karty graficznej. Rozmiar macierzy jest zawsze wielokrotnością rozmiaru bloku wątków, natomiast wielkość bloku wątków (64 lub 256) gwarantuje optymalny podział wątków na 32-wątkowe wiązki. Dzięki temu kod nie musi zawierać instrukcji warunkowych, sprawdzających czy dany wątek ma pracę. Wszystkie wątki wiązki zawsze wykonują w jednej chwili tę samą operację, a zatem rozbieżność przetwarzania wątków wiązki nie występuje.

W przypadku rozmiaru bloku BS~=~16~x~16, aby w pełni wykorzystać możliwości karty, instancja powinna mieć rozmiar, dla którego liczba wszystkich bloków będzie wielokrotnością liczby 108. Wynika to z faktu, że jeden multiprocesor może przetwarzać współbieżnie (przełączając wiązki) 1024 wątki, co daje cztery 256-wątkowe bloki na każdy z 27 multiprocesorów. Instancja  n~=~224, BS~=~16~x~16 zostanie podzielona na 196 bloków, dlatego też, w pewnym momencie obliczeń, niektóre mutliprocesory nie będą w pełni zajęte. Liczba bloków równa 216 zapewniłaby optymalne wykorzystanie zasobów karty, jednak nie da się stworzyć macierzy kwadratowej, która dzieliłaby się na tyle bloków. Można zatem przyjąć, że średnie teoretyczne wykorzystanie zasobów karty graficznej wynosi $196 / 216 \approx 91\% $. Analogicznie dla instancji  n~=~1120, BS~=~16~x~16, uzyskamy wartość $4900 / 4968 \approx 99\% $

W przypadku rozmiaru bloku  BS~=~8~x~8, liczba bloków przydzielonych na jeden multiprocesor powinna wynosić 16. Niestety, dla compute capability 1.3 maksymalna liczba bloków aktywnych jednocześnie na multiprocesorze wynosi 8. Dlatego też multiprocesor będzie przetwarzał maksymalnie 512 wątków, przez co jego zajętość wyniesie maksymalnie $50\%$.  Uwzględniając fakt, że liczba bloków nie jest równa wielokrotności 216 (8 bloków na każdy z 27 multiprocesorów), średnie teoretyczne wykorzystanie zasobów całej karty wyniesie ok. $45\%$ dla instancji n~=~224 oraz n~=~1120.

Eksperyment nie obejmuje obliczeń z wykorzystaniem bloków wątków o wymiarach 32~x~32, ponieważ wykorzystana karta graficzna nie pozwala na stworzenie bloków zawierających więcej niż 512 wątków.


\subsubsection*{Liczba obliczanych macierzy}

Skrócenie czasu przetwarzania w wersji 5 (zrównoleglenie obliczeń i transferu danych) jest możliwe, jeśli wykorzystamy więcej niż jeden strumień. Eksperyment przeprowadzono dla 1, 5 i 10 obliczanych macierzy. W przypadku obliczania tylko jednej macierzy, należy spodziewać się braku skrócenia czasu przetwarzania w stosunku do wersji 3 kodu.

\subsection{Mierzone parametry}

Czas przetwarzania każdej instancji był mierzony w kodzie, za pomocą zdarzeń start i stop (na podstawie przykładowych kodów NVIDII). Obejmował on czas kopiowania danych na kartę graficzną, czas przetwarzania kerneli oraz czas kopiowania wyników do pamięci operacyjnej.

Za pomocą programu NVIDIA Visual Profiler dokonano pomiaru zdarzeń związanych z dostępem do pamięci. Metryki potrzebne do zbadania efektywności to: 
\begin{itemize}
\item \verb|gld 32B| -- liczba 32 bajtowych transakcji pobierania danych z pamięci globalnej,
\item \verb|gld 64B| -- liczba 64 bajtowych transakcji pobierania danych z pamięci globalnej,
\item \verb|gst 32B| -- licznik 32 bajtowych transakcji zapisu danych do pamięci globalnej, każda transakcja powoduje inkrementację licznika o 2,
\item \verb|gst 64B| -- licznik 64 bajtowych transakcji zapisu danych do pamięci globalnej, każda transakcja powoduje inkrementację licznika o 4.
\end{itemize}

Ponadto w celu obliczenia wartości miar \verb|gld efficiency| oraz \verb|gst efficiency| potrzebne są także liczniki  \verb|gld request| oraz  \verb|gst request|. Niestety z niewyjaśnionych przyczyn program NVIDIA Visual Profiler nie był w stanie zebrać tych  miar (wartości dla wszystkich testowanych instancji były równe 0). W przypadku realizowanego zadania miary te nie są jednak kluczowe, ponieważ celem było zbadanie przyspieszenia przetwarzania dzięki zrównolegleniu obliczeń oraz transferów między pamięcią operacyjną a globalną karty.


\subsection{Wyniki eksperymentu}

TODO\\
jeśli będą pasować to wrzucić ty przykładowe screeny, tabele na pewno z czasem przetwarzania, być może też z innymi, niezerowymi parametrami (gld, gst)

\subsection{Wzory}

TODO
\textit{wzory zastosowane do obliczeń wszystkich prezentowanych miar efektywności, znaczenie i zakres wartości
miar efektywności,}

\subsection{Wyniki}

\textit{najważniejsze wyniki w postaci tabelarycznej (oraz wykresów - opcjonalnie dla lepszej wizualizacji), tabele (i
wykresy) należy ponumerować i podpisać w sposób nie budzący wątpliwości co do zawartości, analiza
poprawności prezentowanych wartości i ich zależności od wielkości instancji – czy wartości mieszczą się w
dopuszczalnym zakresie.}

TODO zebrać wyniki z profilera w tabelce, ale one się powtarzają, więc pogrupować.

Warto zwrócić uwagę na wskaźnik \verb|warp serialize| (TODO albo wstawić tabelę albo "wartości znajdują się w załączonym arkuszu z wynikami pomiarów (TODO nazwa pliku)"). Określa on liczbę wiązek, dla których dostęp do pamięci współdzielonej musiał odbywać się sekwencyjnie z powodu konfliktu w dostępie do banków pamięci. W przypadku omawianego kodu, konflikt nigdy nie powinien występować, zarówno w przypadku wykorzystania bloków 16~x~16 jaki i 8~x~8. Wartość tego wskaźnika jest jednak niezerowa w przypadku wszystkich instancji, gdzie BS = 8~x~8. Trudno wyjaśnić co wpłynęło na taki wynik, być może wątki w przypadku instancji z tym rozmiarem bloku były szeregowane w inny sposób, przez co konflikty w dostępie były możliwe.

\section{Wnioski}

TODO

\textit{wnioski z wykonanych eksperymentów z uzasadnieniem (CGMA, zajętość multiprocesora, łączenie dostępów,
rozbieżność przetwarzania, lokalność dostępu do danych, synchronizacja wątków, ilość pracy wątków)
obserwowanych wartości (czytelne odwołanie do omawianej wielkości - gdzie się znajduje i jakiego
uruchomienia - system, parametry, wersja kodu – dotyczą).}\\
-----------------------------------------

\subsection{Dostęp do pamięci globalnej}

Wartości miar \verb|gld| oraz \verb|gst| zebranych przez NVIDIA Visual Profiler wskazują, że zgodnie z przeprowadzoną wcześniej analizą, wszystkie dostępy do pamięci globalnej były łączone w transakcje. W przypadku bloków o rozmiarze 16~x~16 wszystkie dostępy (pobieranie danych oraz zapis wyników) były przeprowadzane w transakcjach 64 bajtowych (tylko wartości \verb|gld 64B| oraz \verb|gst 64B| są niezerowe). Oznacza to, że dostęp do pamięci globalnej był przeprowadzany w optymalny sposób (64B to 16 zmiennych typu float, czyli zawsze dostępy połowy wiązki były łączone w jedną transakcję).

W przypadku wszystkich instancji gdzie bloki wątków miały rozmiar 8~x~8, tylko wartości \verb|gld 32B| oraz \verb|gst 32B| miały wartości niezerowe. Oznacza to, że żądania każdych ośmiu wątków wiązki były łączone w jedną transakcje. Jest to konsekwencją tego, że wątki jednego bloku pobierają lub zapisują dane do fragmentu 8~x~8 pełnej macierzy. Oznacza to, że tylko 8 komórek ma sąsiednie adresy (wiersz bloku), natomiast kolejne 8 wątków pobiera oddalone dane z następnego wiersza macierzy. Z tego powodu nie jest możliwe łączenie dostępów w transakcje 64 bajtowe.

Dostęp do pamięci w przypadku wykorzystania bloków wątków 8~x~8 wciąż jest bardziej efektywny, niż w przypadku gdyby łączenie nie było w ogóle możliwe, jednak dla bloków 16~x~16 można uzyskać optymalną efektywność dostępów do pamięci globalnej. Dodatkowo, wykorzystanie mniejszych bloków pozwala osiągnąć zajętość multiprocesora równą maksymalnie 50\%. Z tego powodu, w przypadku chęci maksymalizacji efektywności przetwarzania wykorzystanie mniejszych bloków nie ma sensu.

\subsection{Zajętość multiprocesora}

W celu zapewnienia optymalnej efektywności przetwarzanie, należy maksymalizować zajętość wszystkich multiprocesorów karty. Dla instancji n~=~224 oraz n~=~1120 oraz obu rozważanych rozmiarów bloku wątków analiza zajętości została przeprowadzona już w punkcie 3.2. Dwie pozostałe instancje n~=~864 oraz n~=~1728, zostały wybrane, ponieważ umożliwiają osiągnięcie  100\% teoretycznej zajętości wszystkich multiprocesorów, w przypadku bloków 16~x~16. W przypadku mniejszych bloków, zajętość procesorów osiągnie 50\%, ograniczeniem jest tutaj limit 8 bloków na multiprocesor.

W przypadku każdej testowanej instancji żądania zasobowe dotyczące liczby rejestrów oraz pamięci współdzielonej były na tyle małe, że nie stanowiły ograniczenia dla zajętości multiprocesorów.
%TODO opisać tu dokładne wartości, jeśli uda się wykminić, jest takie wymaganie w opisie projektu

\subsection{CGMA}

CGMA (Compute to Global Memory Access ratio) jest współczynnikiem określającym intensywność obliczeń, to znaczy ile operacji arytmetycznych (w przypadku mnożenia macierzy będą to operacje dodawania i mnożenia) przypada na dostęp do danych w pamięci globalnej. W przypadku mnożenia macierzy z wykorzystaniem pamięci globalnej do przechowywania wielokrotnie wykorzystywanych bloków macierzy A i B, CGMA będzie równe rozmiarowi bloku (BS). Wynika to z faktu, że każdy wątek bloku pobiera do pamięci współdzielonej po jednej wartości z macierzy A oraz B. Następnie dany wątek w pętli wewnętrznej wykorzystuje jeden wiersz bloku macierzy A oraz jedną kolumnę bloku macierzy B, a zatem wykonuje dwie operacje arytmetyczne BS razy, samemu pobierając wcześniej z pamięci globalnej tylko 2 wartości.

\subsection{Skrócenie czasu przetwarzania wersji 5 kodu}

\subsubsection{Wpływ liczby obliczanych macierzy}

W przypadku obliczania tylko jednej macierzy spodziewanym wynikiem jest brak przyspieszenia, ponieważ nie da się zrównoleglić przesyłania danych i obliczeń. Czas przetwarzania wersji 5 kodu był jednak prawie zawsze krótszy. Jest to spowodowane różnym sposobem alokacji pamięci hosta. Wersja 5 wymagała pamięci z wyłączonym stronicowaniem. Okazuje się jednak, że transfer danych zaalokowanych w ten sposób zawsze (TODO a może jednak nie zawsze tylko zwykle?) trwa nieco krócej. \textit{TODO opisać że alokacja jest wolniejsza, i nie należy nadużywać, bo zamula system (albo jakoś tak, to co gdzieś czytałem).}. Nie należy zatem podczas analizy wpływu innych parametrów brać pod uwagę instancji, gdzie obliczana była tylko jedna macierz.

TODO reszta wniosków do tej sekcji.

\subsubsection{Wpływ wielkości bloku wątków}

Zgonie z wykresem (TODO ref) wykorzystanie większego rozmiaru bloku przekłada się na większy zysk ze zrónoleglenia obliczeń i przesyłania danych. (TODO sprawdzić ten wyjątek). Jest to zgodne z oczekiwaniami, ponieważ czas przetwarzania obliczeń w przypadku bloków 8~x~8 był zawsze dłuższy (z powodu małej zajętości multiprocesorów). Z tego powodu koszt przesyłania danych stanowi większy procent łącznego czasu przetwarzania dla BS~=~16~x~16. Ukrycie kosztów transferu danych przynosi zatem większe zyski.

\subsubsection{Wpływ wielkości pojedynczej macierzy}

Analizując wykres (TODO ref) trudno dostrzec wyraźną zależność skrócenia czasu od rozmiaru macierzy. Biorąc pod uwagę fakt, że koszt przesyłania danych stanowi większy procent czasu w przypadku mniejszych instancji (trzeba przesłać łącznie $3n^2$ danych, natomiast złożoność obliczeniowa wynosi $2n^3$, należy się spodziewać większych zysków w przypadku mniejszych macierzy. Wyniki dla instancji n~=~864 oraz n~=~1728, potwierdzają te przypuszczenia. Dwie pozostałe instancje nie potwierdzają tej tezy, jednak w tym przypadku wyniki były są zaburzone przez fakt, że charakteryzowały się one różną zajętością multiprocesorów. Szczególny przypadek stanowi instancja n~=~224, gdzie obliczenia trwały krócej, niż przesyłanie danych. (!!!!TODO opisać ten dziwny przypadek, czemu gdzieś nastąpiło spowolnienie)



\end{document}